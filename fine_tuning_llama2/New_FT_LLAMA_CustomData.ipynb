{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2f5f098-32b2-4485-a101-9b538c90b0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in c:\\users\\chira\\miniforge3\\envs\\env1\\lib\\site-packages (0.43.1)\n",
      "Requirement already satisfied: torch in c:\\users\\chira\\miniforge3\\envs\\env1\\lib\\site-packages (from bitsandbytes) (2.3.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\chira\\miniforge3\\envs\\env1\\lib\\site-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\chira\\miniforge3\\envs\\env1\\lib\\site-packages (from torch->bitsandbytes) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\chira\\miniforge3\\envs\\env1\\lib\\site-packages (from torch->bitsandbytes) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\chira\\miniforge3\\envs\\env1\\lib\\site-packages (from torch->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\chira\\miniforge3\\envs\\env1\\lib\\site-packages (from torch->bitsandbytes) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\chira\\miniforge3\\envs\\env1\\lib\\site-packages (from torch->bitsandbytes) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\chira\\miniforge3\\envs\\env1\\lib\\site-packages (from torch->bitsandbytes) (2024.5.0)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\chira\\miniforge3\\envs\\env1\\lib\\site-packages (from torch->bitsandbytes) (2021.4.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\chira\\miniforge3\\envs\\env1\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->bitsandbytes) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\chira\\miniforge3\\envs\\env1\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->bitsandbytes) (2021.13.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\chira\\miniforge3\\envs\\env1\\lib\\site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\chira\\miniforge3\\envs\\env1\\lib\\site-packages (from sympy->torch->bitsandbytes) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95a16509-6f89-4540-ae81-76a89c3945e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q accelerate==0.21.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdc17f15-3d4f-4040-b53a-59fc1b3b541a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q peft==0.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5212c4ca-945a-42d9-94a7-08878fdc9d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96fac8df-40d5-4174-87dd-393382c95db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers==4.38.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4db6175-633b-4317-8e25-58e80695c40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q trl==0.4.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022ede48-458e-4a8d-9bcb-b8d6b97a1984",
   "metadata": {},
   "source": [
    "**Accelerate** is a library that enables the same PyTorch code to be run across any distributed configuration by adding just four lines of code!\n",
    "\n",
    "**PEFT** (Parameter-Efficient Fine-Tuning) is a library for efficiently adapting large pretrained models to various downstream applications without fine-tuning all of a modelâ€™s parameters because it is prohibitively costly. PEFT methods only fine-tune a small number of (extra) model parameters - significantly decreasing computational and storage costs - while yielding performance comparable to a fully fine-tuned model.\n",
    "\n",
    "**bitsandbytes** enables accessible large language models via k-bit quantization for PyTorch. bitsandbytes provides three main features for dramatically reducing memory consumption for inference and training\n",
    "\n",
    "**TRL** is a full stack library where we provide a set of tools to train transformer language models with Reinforcement Learning, from the Supervised Fine-tuning step (SFT), Reward Modeling step (RM) to the Proximal Policy Optimization (PPO) step. The library is integrated with ðŸ¤— transformers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bd9e10-65fd-4720-beab-10143e3f4835",
   "metadata": {},
   "source": [
    "## Step: Import all the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55b4e5e5-c50c-48fd-9317-3b22a77776c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch \n",
    "import accelerate\n",
    "from datasets import load_dataset\n",
    "from transformers import(\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer, \n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122381eb-c778-4929-8bf2-570223a79a66",
   "metadata": {},
   "source": [
    "## In case of LLAMA 2, the following prompt template is used for the chat models\n",
    "\n",
    "<s.> [INST] <<SYS.>>\n",
    "\n",
    "System Prompt\n",
    "\n",
    "<</SYS.>>\n",
    "\n",
    "User Prompt [/INST] Model answer </s.>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21d5d4b-8b35-403c-9f5f-e4480399cad8",
   "metadata": {},
   "source": [
    "## We Will reformat our dataset into the LLAMA model format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2318611-6bd4-4979-846e-473b3f74f603",
   "metadata": {},
   "source": [
    "- Orignal Dataset: https://huggingface.co/datasets/timdettmers/openassistant-guanaco\n",
    "  \n",
    "- Reformat Dataset following the Llama 2 template with 1k sample: https://huggingface.co/datasets/mlabonne/guanaco-llama2-1k\n",
    "\n",
    "- Complete Reformat Dataset following the Llama 2 template: https://huggingface.co/datasets/mlabonne/guanaco-llama2\n",
    "  \n",
    "\n",
    "To know how this dataset was created, you can check this notebook.\n",
    "\n",
    "https://colab.research.google.com/drive/1Ad7a9zMmkxuXTOh1Z7-rNSICA4dybpM2?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb1d43a-52e7-4b1e-bdf9-2ae766ebd7d5",
   "metadata": {},
   "source": [
    "### Note: You donâ€™t need to follow a specific prompt template if youâ€™re using the base Llama 2 model instead of the chat version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfc6c84-971b-4ced-a848-3c44c984ab12",
   "metadata": {},
   "source": [
    "## How to fine tune LLAMA 2\n",
    "\n",
    "- Free Resources are barely enough to store LLAMA 2 - 7b weights\n",
    "- We also need to consider the overhead due to optimizer states, gradients, and forward activations\n",
    "- Full fine-tuning is not possible here: we need parameter-efficient fnue-tuning (PEFT) techniques like LoRA.\n",
    "- To drastically reduce the VRAM usage, we must fine-tune the model in 4-bit precision, which is why we use LoRA here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfd009e-3e17-4487-a935-02f5d1c3b052",
   "metadata": {},
   "source": [
    "## Step 3\n",
    "\n",
    "1. Load a LLAMA-2-7b-chat-hf model (chat model)\n",
    "2. Train it on the mlabonne/guanaco-llama2-1k (1000samples), which will produce our fine-tuned model LLAMA-2-7b-chat-finetune\n",
    "\n",
    "QLoRA will use a rank of 64 with a scaling parameter of 16. We'll load the Llama 2 model directly in 4-bit precision using the NF4 type and train it for one epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "762c0026-fffc-42fc-953c-88ff4caf2b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model that we are training from the hugging face hub\n",
    "model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# The Instruction dataset to use\n",
    "dataset_name = \"mlabonne/guanaco-llama2-1k\"\n",
    "\n",
    "# Fine-tuned model name\n",
    "new_model = \"Llama-2-7b-chat-finetune\"\n",
    "\n",
    "############################################################################################\n",
    "# QLoRA parameters\n",
    "############################################################################################\n",
    "\n",
    "# LoRA attention dimension\n",
    "lora_r = 64\n",
    "\n",
    "# Aplha parameter for LoRA scaling\n",
    "lora_alpha = 16 \n",
    "\n",
    "#Dropout probability for LoRA layers \n",
    "lora_dropout = 0.1\n",
    "\n",
    "############################################################################################\n",
    "# bitsandbytes parameters \n",
    "############################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_type = \"float16\"\n",
    "\n",
    "# Quantization Type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "\n",
    "############################################################################################\n",
    "# TrainingArguments Parameters\n",
    "############################################################################################\n",
    "\n",
    "\n",
    "# Output directory where the model predictions and checkpoints will be stored \n",
    "output_dir = \"./results\"\n",
    "\n",
    "# Number of training epochs\n",
    "num_train_epochs = 1\n",
    "\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = False\n",
    "bf16 = False\n",
    "\n",
    "# Batch size per GPU for training \n",
    "per_device_train_batch_size = 4\n",
    "\n",
    "# Batch size per GPU for evaluation\n",
    "per_device_eval_batch_size = 4\n",
    "\n",
    "# Number of updates steps to accumulate the gradients for \n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "# Enable gradient Checkpointing \n",
    "gradient_checkpointing = True\n",
    "\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.3\n",
    "\n",
    "# Initial Learning Rate (AdamW optimizer)\n",
    "learning_rate = 2e-4\n",
    "\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights \n",
    "weight_decay = 0.001\n",
    "\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "\n",
    "# Learning rate schedule\n",
    "lr_scheduler_type = \"cosine\"\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = -1\n",
    "\n",
    "# Ratio of steps for linear warmup (from 0 to learning rate)\n",
    "warup_ratio = 0.03\n",
    "\n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = True\n",
    "\n",
    "# Save checkpoint every X updates steps \n",
    "save_steps = 0\n",
    "\n",
    "# log every X updates steps\n",
    "logging_steps = 25\n",
    "\n",
    "############################################################################################\n",
    "# SFT parameters\n",
    "############################################################################################\n",
    "\n",
    "# Maximum sequence length to use\n",
    "max_seq_length = None \n",
    "\n",
    "# pack multiple short example in the same input sequence to increase efficiency\n",
    "packing = False\n",
    "\n",
    "# Load the entire model on the GPU 0\n",
    "device_map = {\"\":0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0517e37-7f05-480f-8b00-63be61dcec76",
   "metadata": {},
   "source": [
    "## Step 4 : Load Everything and start the fine tuning process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c41603c-7f2b-4151-aa30-073c7bb01516",
   "metadata": {},
   "source": [
    "1. First we need to load the dataset we defined. In this case, our data is already preprocessed, we reformat the prompt, filter out bad text, combine multiple datasets, etc.\n",
    "\n",
    "2. Then, we're configuring bitsandbytes for 4-bit quantization\n",
    "\n",
    "3. Next, loading the Llama 2 model in 4-bit precision on a GPU with the corresponding tokenizer.\n",
    "\n",
    "4. Finally, loading the configuration for QLoRA, regular training parameters, and pass everything to the SFTTrainer. Now, training can finally start!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73a9b364-b6fa-44e9-83d8-005860e079a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in c:\\users\\chira\\miniforge3\\envs\\env1\\lib\\site-packages (0.32.1)\n",
      "Requirement already satisfied: bitsandbytes in c:\\users\\chira\\miniforge3\\envs\\env1\\lib\\site-packages (0.43.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.17 in c:\\users\\chira\\miniforge3\\envs\\env1\\lib\\site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\chira\\miniforge3\\envs\\env1\\lib\\site-packages (from accelerate) (24.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\chira\\miniforge3\\envs\\env1\\lib\\site-packages (from accelerate) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\chira\\miniforge3\\envs\\env1\\lib\\site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\chira\\miniforge3\\envs\\env1\\lib\\site-packages (from accelerate) (2.3.1)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\chira\\miniforge3\\envs\\env1\\lib\\site-packages (from accelerate) (0.24.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\chira\\miniforge3\\envs\\env1\\lib\\site-packages (from accelerate) (0.4.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\chira\\miniforge3\\envs\\env1\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\chira\\miniforge3\\envs\\env1\\lib\\site-packages (from torch>=1.10.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\chira\\miniforge3\\envs\\env1\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\chira\\miniforge3\\envs\\env1\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\chira\\miniforge3\\envs\\env1\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\chira\\miniforge3\\envs\\env1\\lib\\site-packages (from torch>=1.10.0->accelerate) (2024.5.0)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\chira\\miniforge3\\envs\\env1\\lib\\site-packages (from torch>=1.10.0->accelerate) (2021.4.0)\n",
      "Requirement already satisfied: requests in c:\\users\\chira\\miniforge3\\envs\\env1\\lib\\site-packages (from huggingface-hub->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\chira\\miniforge3\\envs\\env1\\lib\\site-packages (from huggingface-hub->accelerate) (4.66.4)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\chira\\miniforge3\\envs\\env1\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.10.0->accelerate) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\chira\\miniforge3\\envs\\env1\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.10.0->accelerate) (2021.13.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\chira\\miniforge3\\envs\\env1\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\chira\\miniforge3\\envs\\env1\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\chira\\miniforge3\\envs\\env1\\lib\\site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\chira\\miniforge3\\envs\\env1\\lib\\site-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\chira\\miniforge3\\envs\\env1\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\chira\\miniforge3\\envs\\env1\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2024.7.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\chira\\miniforge3\\envs\\env1\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade accelerate bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27106d96-781c-4a98-9ec3-e19ebc6ffb1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21.0\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Using `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(accelerate\u001b[38;5;241m.\u001b[39m__version__)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Load base model \u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     31\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\miniforge3\\envs\\env1\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:561\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    560\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    562\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    563\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    565\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    566\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    567\u001b[0m )\n",
      "File \u001b[1;32m~\\miniforge3\\envs\\env1\\Lib\\site-packages\\transformers\\modeling_utils.py:3024\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   3021\u001b[0m     hf_quantizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 3024\u001b[0m     \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3025\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\n\u001b[0;32m   3026\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3027\u001b[0m     torch_dtype \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_torch_dtype(torch_dtype)\n\u001b[0;32m   3028\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_device_map(device_map)\n",
      "File \u001b[1;32m~\\miniforge3\\envs\\env1\\Lib\\site-packages\\transformers\\quantizers\\quantizer_bnb_4bit.py:62\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.validate_environment\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate_environment\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_accelerate_available() \u001b[38;5;129;01mand\u001b[39;00m is_bitsandbytes_available()):\n\u001b[1;32m---> 62\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     63\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     64\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     65\u001b[0m         )\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_tf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_flax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     69\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConverting into 4-bit or 8-bit weights from tf/flax weights is currently not supported, please make\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     70\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m sure the weights are in PyTorch format.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     71\u001b[0m         )\n",
      "\u001b[1;31mImportError\u001b[0m: Using `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`"
     ]
    }
   ],
   "source": [
    "# Load dataset (you can process it here)\n",
    "dataset = load_dataset(dataset_name, split = \"train\")\n",
    "\n",
    "# load tokenizer and model with QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_type)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = use_4bit, \n",
    "    bnb_4bit_quant_type = bnb_4bit_quant_type, \n",
    "    bnb_4bit_compute_dtype = compute_dtype,\n",
    "    bnb_4bit_use_double_quant = use_nested_quant,\n",
    ")\n",
    "\n",
    "# # Check GPU compatibility with bfloat16\n",
    "# if compute_dtype ==torch.float16 and use_4bit:\n",
    "#     major, _ = torch.cuda.get_device_capability()\n",
    "#     if major >= 8:\n",
    "#         print(\"=\" * 80)\n",
    "#         print(\"Your GPU supports bfloat16 : accelerate training with bf16=True\")\n",
    "#         print(\"=\" * 80)\n",
    "\n",
    "\n",
    "print(accelerate.__version__)\n",
    "# Load base model \n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    quantization_config = bnb_config,\n",
    "    device_map = device_map\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Load LLaMa tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code = True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" # Fix weird overflow issue fp16 training\n",
    "\n",
    "# Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha = lora_alpha,\n",
    "    lora_dropout = lora_dropout,\n",
    "    r = lora_r,\n",
    "    bias = \"none\",\n",
    "    task_type = \"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Set training parameters \n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir = output_dir, \n",
    "    num_train_epochs = num_train_epochs, \n",
    "    per_device_train_batch_size = per_device_train_batch_size, \n",
    "    gradient_accumulation_steps = gradient_accumulation_steps,\n",
    "    optim=optim, \n",
    "    save_steps = save_steps,\n",
    "    logging_steps = logging_steps, \n",
    "    learning_rate = learning_rate, \n",
    "    weight_decay = weight_decay,\n",
    "    fp16 = fp16, \n",
    "    bf16 = bf16, \n",
    "    max_grad_norm = max_grad_norm, \n",
    "    max_steps = max_steps, \n",
    "    warmup_ratio = warmup_ratio, \n",
    "    group_by_length = group_by_length, \n",
    "    lr_scheduler_type = lr_scheduler_type, \n",
    "    report_to = \"tensorboard\"\n",
    ")\n",
    "\n",
    "# Set supervised fine-tuning parameters \n",
    "trainer = SFTTrainer(\n",
    "    model = model, \n",
    "    train_dataset = dataset, \n",
    "    peft_config = peft_config, \n",
    "    dataset_text_field = \"text\", \n",
    "    max_seq_length = max_seq_length, \n",
    "    tokenizer = tokenizer, \n",
    "    args = training_arguments, \n",
    "    packing = packing,\n",
    ")\n",
    "\n",
    "# Train Model \n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15251adc-b32f-483d-a5b4-637918128f4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da4ce6b-c7b7-40c7-823c-64bc407883d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680158f9-14a5-4ff2-8528-9d7ed9d0d093",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4311db-4c1d-4625-93db-732dc095ab4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3250bc9-cee4-414b-bcbf-c584a59f0692",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2d701b-a19e-431a-a862-559385c95fa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856d1ad7-432f-4e08-9372-89ad02f5ee78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "env1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
